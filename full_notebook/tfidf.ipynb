{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ff5799-9444-4c0e-9ba0-33685af0b17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-05T21:55:25.402164Z",
     "iopub.status.busy": "2022-12-05T21:55:25.401348Z",
     "iopub.status.idle": "2022-12-05T21:55:38.792854Z",
     "shell.execute_reply": "2022-12-05T21:55:38.792260Z",
     "shell.execute_reply.started": "2022-12-05T21:55:25.402135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/steven/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as pp \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.utils import shuffle \n",
    "import regex \n",
    "%pip install emoji\n",
    "import emoji\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import random \n",
    "import time \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%pip install transformers\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, BertConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, SequentialSampler, RandomSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "RANDOM_STATE = 256\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f8234e-2b96-45f7-8106-a5359482b056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-05T21:57:40.499295Z",
     "iopub.status.busy": "2022-12-05T21:57:40.498972Z",
     "iopub.status.idle": "2022-12-05T21:57:40.655408Z",
     "shell.execute_reply": "2022-12-05T21:57:40.654337Z",
     "shell.execute_reply.started": "2022-12-05T21:57:40.499273Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    preprocess: takes path to single tweets.csv and cleans it\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    - path: path to .csv file\n",
    "    \n",
    "    returns: \n",
    "    - x: cleaned data \n",
    "    - y: labels \n",
    "    \n",
    "    use: only use on original train/val csv from kaggle \n",
    "    \n",
    "    \"\"\" \n",
    "\n",
    "    dat = pd.read_csv(path)\n",
    "    add_row = dat.columns.to_numpy()\n",
    "    dat.loc[len(dat.index)] = add_row\n",
    "    dat.columns = [\"Index\", \"Source\", \"Sentiment\", \"Tweet\"]\n",
    "\n",
    "    x = pd.DataFrame(dat[\"Tweet\"])\n",
    "    y = pd.DataFrame(dat[\"Sentiment\"])\n",
    "\n",
    "    y.drop(dat[dat.isnull().any(axis=1)].index.to_numpy(), inplace=True)\n",
    "    y.reset_index(inplace=True)\n",
    "    y.drop(\"index\", axis=1, inplace=True)\n",
    "    x.drop(dat[dat.isnull().any(axis=1)].index.to_numpy(), inplace=True)\n",
    "    x.reset_index(inplace=True)\n",
    "    x.drop(\"index\", axis=1, inplace=True)\n",
    "    \n",
    "    my_dict = {'positive':0, 'negative':1, 'neutral':2,'irrelevant':3}\n",
    "    \n",
    "\n",
    "    y = y.to_numpy()\n",
    "    y[y == 'Positive'] = 0 \n",
    "    y[y == 'Negative'] = 1\n",
    "    y[y == 'Neutral'] = 2\n",
    "    y[y == 'Irrelevant'] = 3\n",
    "    x = x.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "    x = x.flatten()\n",
    "    y = y.astype(int).flatten()\n",
    "\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def train_val_test(x, y, first_split=0.40, second_split=0.50, seed=256):\n",
    "    \n",
    "    \"\"\"\n",
    "    train_val_test: sped up function for train/test/splitting of cleaned .npy data \n",
    "    \n",
    "    params: \n",
    "    - x: cleaned tweet.npy \n",
    "    - y: cleaned label.npy \n",
    "    - first_split: test split proportion for first split (train/unprocessed) \n",
    "    - second_split: test split proportion for second split of unprocessed (val/test)\n",
    "    - seed: random seed for splitting \n",
    "    \n",
    "    returns: \n",
    "    - dict of train/val/tst sets \n",
    "    \n",
    "    \"\"\" \n",
    "    np.random.seed(seed)\n",
    "    x_tr, x_mid, y_tr, y_mid = train_test_split(\n",
    "        x, y, test_size=first_split, random_state=seed\n",
    "    )\n",
    "    x_val, x_tst, y_val, y_tst = train_test_split(\n",
    "        x_mid, y_mid, test_size=second_split, random_state=seed\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"x_tr\": x_tr,\n",
    "        \"y_tr\": y_tr,\n",
    "        \"x_val\": x_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"x_tst\": x_tst,\n",
    "        \"y_tst\": y_tst,\n",
    "    }\n",
    "\n",
    "\n",
    "def to_loader(output, batch_size=32, type=\"tr\"):\n",
    "    \n",
    "    \"\"\" \n",
    "    to_loader: \n",
    "    - compiles tokenized data into tensor datasets\n",
    "    - turns tensor datasets into torch data loaders of specified batch_size\n",
    "    \n",
    "    params: \n",
    "    - output: len(3) tuple resulting from @tokenize function or regular (dat,lab) tuple  \n",
    "    - batch_size: batch size for dataloaders \n",
    "    - type: must be in ['tr', 'val', 'tst'] \n",
    "        -- determines what type of random sampler to use \n",
    "        \n",
    "    returns: \n",
    "    - dataloader: torch dataloader  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(output) == 3: \n",
    "    \n",
    "        dat = TensorDataset(output[0], output[1], output[2])\n",
    "        \n",
    "        if type == \"tr\":\n",
    "            dataloader = DataLoader(dat, sampler=RandomSampler(dat), batch_size=batch_size)\n",
    "            return dataloader\n",
    "        \n",
    "        elif type in [\"val\", \"tst\"]:\n",
    "            dataloader = DataLoader(\n",
    "                dat, sampler=SequentialSampler(dat), batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            return dataloader\n",
    "        \n",
    "    elif len(output) == 2: \n",
    "        \n",
    "        dat = TensorDataset(output[0], output[1])\n",
    "        \n",
    "        if type == \"tr\":\n",
    "            dataloader = DataLoader(dat, sampler=RandomSampler(dat), batch_size=batch_size)\n",
    "            return dataloader\n",
    "        \n",
    "        elif type in [\"val\", \"tst\"]:\n",
    "            dataloader = DataLoader(\n",
    "                dat, sampler=SequentialSampler(dat), batch_size=batch_size\n",
    "            )\n",
    "            return dataloader\n",
    "    \n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.EMOJI_DATA for char in word):\n",
    "            emoji_list.append(word)\n",
    "    flags = regex.findall(u'[\\U0001F1E6-\\U0001F1FF]', text)\n",
    "\n",
    "    return emoji_list + flags\n",
    "\n",
    "\n",
    "def get_emoji(x): \n",
    "    emoji_list = [] \n",
    "    for i in range(0, len(x)): \n",
    "        text = x[i]\n",
    "        emojis = split_count(text) \n",
    "        if len(emojis) > 0: \n",
    "            emoj_list.append(list(set(emojis)))\n",
    "    emoj_flat = [item for sublist in emoj_list for item in sublist]\n",
    "\n",
    "    return emoj_flat\n",
    "\n",
    "\n",
    "def tokenize(x, y, tokenizer, emoji = False):\n",
    "\n",
    "        \"\"\" \n",
    "        tokenize: takes in .numpy data and tokenizes using BertTokenizer \n",
    "\n",
    "        params: \n",
    "        - x: ,npy tweets\n",
    "        - y: .npy labels \n",
    "\n",
    "        returns: \n",
    "        - input_ids: id tokens \n",
    "        - attention_masks: masks \n",
    "        - labels: labels \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if emoji is False: \n",
    "\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "            for sentence in x:\n",
    "                encoded_dict = tokenizer.encode_plus(\n",
    "                    sentence,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=64,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                )\n",
    "                input_ids.append(encoded_dict[\"input_ids\"])\n",
    "                attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "            input_ids = torch.cat(input_ids, dim=0).cuda()\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).cuda()\n",
    "            labels = torch.tensor(y).type(torch.LongTensor).cuda()\n",
    "\n",
    "            return (input_ids, attention_masks, labels)\n",
    "\n",
    "        else: \n",
    "\n",
    "            emojis = get_emoji(x) \n",
    "            tokenizer.add_tokens(emojis)\n",
    "\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "            for sentence in x:\n",
    "                encoded_dict = tokenizer.encode_plus(\n",
    "                    sentence,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=64,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                )\n",
    "                input_ids.append(encoded_dict[\"input_ids\"])\n",
    "                attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "            input_ids = torch.cat(input_ids, dim=0).cuda()\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).cuda()\n",
    "            labels = torch.tensor(y).type(torch.LongTensor).cuda()\n",
    "\n",
    "            return (input_ids, attention_masks, labels)\n",
    "\n",
    "\n",
    "def data_pipe(X, Y, tokenizer, first_split=0.40, second_split=0.50, seed=256, batch_size=32, emoji = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    data_pipe: splits, tokenizes, and returns data in dataloaders \n",
    "    \n",
    "    params: \n",
    "    - X: tweet.npy \n",
    "    - Y: label.npy \n",
    "    - first_split: see @train_val_test \n",
    "    - second_split: see @train_val_test \n",
    "    - seed: see @train_val_test \n",
    "    - batch_size: see @to_loader\n",
    "    \n",
    "    returns: \n",
    "    - train_loader: loader for train dat \n",
    "    - val_loader: loader for val dat \n",
    "    - tst_loader: loader for tst dat\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    splits = train_val_test(X, Y, first_split, second_split, seed)\n",
    "    tokenizer = tokenizer \n",
    "    train_token = tokenize(splits[\"x_tr\"], splits[\"y_tr\"], tokenizer, emoji)\n",
    "    val_token = tokenize(splits[\"x_val\"], splits[\"y_val\"], tokenizer, emoji)\n",
    "    tst_token = tokenize(splits[\"x_tst\"], splits[\"y_tst\"], tokenizer, emoji)\n",
    "\n",
    "    train_loader = to_loader(train_token, batch_size, type = \"tr\")\n",
    "    val_loader = to_loader(val_token, batch_size, type = \"val\")\n",
    "    tst_loader = to_loader(tst_token, batch_size, type = \"tst\")\n",
    "\n",
    "    return (train_loader, val_loader, tst_loader)\n",
    "\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    flat_accuracy: computes accuracy between preds and labels \n",
    "    \n",
    "    params: \n",
    "    - preds: predictions\n",
    "    - labels: labels \n",
    "    \n",
    "    returns: \n",
    "    - accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds.numpy(), axis=1).flatten()\n",
    "    labels_flat = labels.numpy().flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    \n",
    "    \"\"\" \n",
    "    f1_score_func: computes weighted f1 between preds and labels \n",
    "    \n",
    "    params: \n",
    "    - preds: predictions\n",
    "    - labels: labels \n",
    "    \n",
    "    returns: \n",
    "    - weighted f1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    preds_flat = np.argmax(preds.numpy(), axis=1).flatten()\n",
    "    labels_flat = labels.numpy().flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=\"weighted\")\n",
    "\n",
    "def finetune(path, model, train_loader, val_loader, tokenizer_length = None,  epochs=5, tolerance=0.01, patience = 2, emoji = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    finetune: finetunes pretrained BertForSequenceClassification model on tweet data \n",
    "    \n",
    "    params: \n",
    "    - path: path to store best model \n",
    "    - model: BertForSequenceClassificationModel \n",
    "    - train_loader: torch loader for train dat \n",
    "    - val_loader: torch loader for val dat \n",
    "    - epochs: number of times to iterate through data \n",
    "    - tolerance: decides how close a lower validation loss can be to a previous best \n",
    "    - patience: decides how long loss can be lower than previous best before function exits \n",
    "    \n",
    "    returns: \n",
    "    - training_stats: dictionary of accuracy, f1, val loss, and training loss over epochs \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = model\n",
    "    \n",
    "    if emoji: \n",
    "        \n",
    "        model.resize_token_embeddings(tokenizer_length)\n",
    "        \n",
    "\n",
    "    train_loader = train_loader\n",
    "    val_loader = val_loader\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    seed_val = 256\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    training_stats = {\"acc\": [], \"f1\": [], \"val_loss\": [], 'training_loss': []}\n",
    "    val_loss_tracker = 2e5\n",
    "    tolerance = tolerance\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].cuda()\n",
    "            b_labels = batch[2].cuda()\n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss = outputs.loss.cuda()\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        training_stats['training_loss'].append(avg_train_loss)\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_f1 = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        for batch in val_loader:\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].cuda()\n",
    "            b_labels = batch[2].cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels,\n",
    "                )\n",
    "                loss = outputs.loss.cuda()\n",
    "                logits = outputs.logits.cuda()\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "            total_eval_f1 += f1_score_func(logits.cpu(), b_labels.cpu())\n",
    "            total_eval_accuracy += flat_accuracy(logits.cpu(), b_labels.cpu())\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "        avg_val_f1 = total_eval_f1 / len(val_loader)\n",
    "        avg_val_loss = total_eval_loss / len(val_loader)\n",
    "        training_stats[\"acc\"].append(avg_val_accuracy)\n",
    "        training_stats[\"f1\"].append(avg_val_f1)\n",
    "        training_stats[\"val_loss\"].append(avg_val_loss)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "        print(\"  F1: {0:.2f}\".format(avg_val_f1))\n",
    "        print(\"  Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        if avg_val_loss < val_loss_tracker - tolerance:\n",
    "            val_loss_tracker = avg_val_loss\n",
    "            torch.save(model.state_dict(), path + \"best_model.pth\")\n",
    "        elif avg_val_loss > val_loss_tracker + tolerance:\n",
    "            print(\"Exiting. Current loss is worst that previous.\")\n",
    "            break\n",
    "            \n",
    "    return training_stats\n",
    "\n",
    "\n",
    "def get_embeddings(model, loader): \n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    get_embeddings: \n",
    "    \n",
    "    - (1) Uses BertModelForSequenceClassification with finetuned state_dict loaded \n",
    "                    to generate hidden state embeddings for sentences\n",
    "    - (2) Uses BertModel with Finetuned state_dict loaded to generate pooled cls token embedding for sentences\n",
    "    \n",
    "    params: \n",
    "    - model: BertModel with Finetuned state_dict loaded or BertModelForSequenceClassification with finetuned state_dict\n",
    "    - loader: desired data to generate embeddings for \n",
    "    \n",
    "    returns: \n",
    "    - embeddings: pooled cls token sentence embeddings or hidden state sentence embeddings\n",
    "    - labels: labels \n",
    "    \n",
    "    see @BertModel https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertModel\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_list = []\n",
    "    label_list = []\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(model, BertForSequenceClassification): \n",
    "\n",
    "\n",
    "        for step, batch in enumerate(loader):\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].cuda()\n",
    "            b_labels = batch[2].cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                hidden = outputs.hidden_states\n",
    "                embedding = hidden[-1][:, 0, :]\n",
    "                embedding_list.append(embedding.cpu())\n",
    "                label_list.append(b_labels.cpu())\n",
    "\n",
    "        embeddings = torch.cat(embedding_list, dim=0)\n",
    "        labels = torch.cat(label_list, dim=0)\n",
    "        \n",
    "        return (embeddings, labels)\n",
    "\n",
    "    \n",
    "    else:  \n",
    "\n",
    "        for step, batch in enumerate(loader):\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].cuda()\n",
    "            b_labels = batch[2].cuda()\n",
    "            with torch.no_grad():\n",
    "                outputs = new_mod(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                )\n",
    "                embedding_list.append(outputs.pooler_output.cpu())\n",
    "                label_list.append(b_labels.cpu())\n",
    "\n",
    "        embeddings = torch.cat(embedding_list, dim=0)\n",
    "        labels = torch.cat(label_list, dim=0)\n",
    "\n",
    "        return (embeddings, labels)\n",
    "    \n",
    "def test_on_batch(model, loader): \n",
    "    \n",
    "    total_eval_f1 = 0\n",
    "    total_eval_acc = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_input_mask = batch[1].cuda()\n",
    "        b_labels = batch[2].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            logits = outputs.logits.cuda()\n",
    "\n",
    "        total_eval_f1 += f1_score_func(logits.cpu(), b_labels.cpu())\n",
    "        total_eval_acc += flat_accuracy(logits.cpu(), b_labels.cpu())\n",
    "        \n",
    "    avg_f1 = total_eval_f1/len(loader)\n",
    "    avg_acc = total_eval_acc/len(loader)\n",
    "    \n",
    "    \n",
    "    return (avg_f1, avg_acc)\n",
    "\n",
    "\n",
    "def plot_tune_loss(stats_dict, title, y_lim = [0.1,1]): \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plot_tune_loss: plots the stats_dict returned from @finetune \n",
    "    \n",
    "    params: \n",
    "    - stats_dict: return from @finetune \n",
    "    - y_lim: y_limits on graph \n",
    "    \n",
    "    returns: \n",
    "    - fig: the plot \n",
    "    \n",
    "    \"\"\"\n",
    "    avg_val_loss = stats_dict['val_loss']\n",
    "    avg_acc = stats_dict['acc']\n",
    "    avg_f1 = stats_dict['f1']\n",
    "    avg_train_loss = stats_dict['training_loss']\n",
    "    epoch_list = range(1, len(avg_val_loss)+1)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    fig = figure(figsize=(15, 15), dpi=800)\n",
    "\n",
    "    plt.rcParams.update({\"font.size\": 22})\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.ylim(y_lim[0], y_lim[1])\n",
    "    lw = 5\n",
    "    \n",
    "    plt.plot(\n",
    "    list(epoch_list),\n",
    "    list(avg_f1),\n",
    "    linewidth=lw,\n",
    "    alpha=0.2,\n",
    "    color= 'darkgreen',\n",
    "    label= \"F1 Score\",\n",
    "    marker=\"o\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        list(epoch_list),\n",
    "        list(avg_acc),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color= 'darkblue',\n",
    "        label= \"Accuracy\",\n",
    "        marker=\"o\",\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "        list(epoch_list),\n",
    "        list(avg_val_loss),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color= 'darkred',\n",
    "        label= \"Validation Loss\",\n",
    "        marker=\"o\",\n",
    "        )\n",
    "    \n",
    "    plt.plot(\n",
    "        list(epoch_list),\n",
    "        list(avg_train_loss),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color= 'purple',\n",
    "        label= \"Training Loss\",\n",
    "        marker=\"o\",\n",
    "        )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_knn(metric_distances, y_lim = [0.1, 1]):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plot_knn: plots the metric dictionary returned from @finetune \n",
    "    \n",
    "    params: \n",
    "    - metric_distances: metric dictionary generated by knn gridsearch\n",
    "    - y_lim: y_limits on graph \n",
    "    \n",
    "    returns: \n",
    "    - fig: the plot \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = figure(figsize=(15, 15), dpi=800)\n",
    "    plt.rcParams.update({\"font.size\": 22})\n",
    "    \n",
    "    if isinstance(metric_distances, dict): \n",
    "    \n",
    "        plt.title(\"Hidden State Features. Across KNN Distance Metrics and NNeighbors\")\n",
    "        plt.xlabel(\"Neighbors\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.ylim(y_lim[0], y_lim[1])\n",
    "        lw = 5\n",
    "\n",
    "        colors = {'euclidean': 'darkred', 'cityblock': 'darkblue', 'cosine':'darkgreen', 'correlation':'purple'}\n",
    "        for j in [\"euclidean\", \"cityblock\", \"cosine\", \"correlation\"]:\n",
    "            plt.plot(\n",
    "            list(range(1,len(metric_distances[j])+1)),\n",
    "            list(metric_distances[j]),\n",
    "            linewidth=lw,\n",
    "            alpha=0.2,\n",
    "            color= colors[j],\n",
    "            label= j,\n",
    "            marker=\"o\",\n",
    "            )\n",
    "\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "        return fig \n",
    "    \n",
    "    if isinstance(metric_distances, list): \n",
    "        \n",
    "        plt.title(\"Sentence Transformer Features Across KNN NNeighbors\")\n",
    "        plt.xlabel(\"Neighbors\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.ylim(y_lim[0], y_lim[1])\n",
    "        lw = 5\n",
    "        plt.plot(\n",
    "            list(range(1,len(metric_distances)+1)),\n",
    "            metric_distances, \n",
    "            linewidth = lw, \n",
    "            alpha = 0.2,\n",
    "            color = \"darkgreen\",\n",
    "            label = \"F1\",\n",
    "            marker = \"o\"\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        \n",
    "        return fig \n",
    "        \n",
    "\n",
    "def plot_svc(score_dict, y_lim = [0.1, 0.9], c = 2, c_range = range(-5,5)): \n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = figure(figsize=(15, 15), dpi=800)\n",
    "\n",
    "    plt.rcParams.update({\"font.size\": 22})\n",
    "    plt.title(\"Hidden State Features. Across SVM Kernel and Regularization\")\n",
    "    plt.xlabel(\"C (Base \" + str(c) + \")\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.ylim(y_lim[0], y_lim[1])\n",
    "    lw = 5\n",
    "\n",
    "    colors = {'poly': 'darkred', 'rbf': 'darkblue', 'sigmoid':'darkgreen', 'linear':'purple'}\n",
    "    for j in [\"poly\", \"rbf\", \"sigmoid\", \"linear\"]:\n",
    "        plt.plot(\n",
    "        list(c_range),\n",
    "        list(score_dict[j]),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color= colors[j],\n",
    "        label= j,\n",
    "        marker=\"o\",\n",
    "        )\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "            \n",
    "    return fig \n",
    "\n",
    "def plot_adaboost(scores, y_lim = [0.1, 0.9], depth = range(10,15)): \n",
    "    %matplotlib inline\n",
    "    \n",
    "    fig = figure(figsize = (15,15), dpi = 800) \n",
    "    \n",
    "    plt.rcParams.update({\"font.size\": 22})\n",
    "    plt.title(\"ADA-BOOST F1 by Depth\")\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.ylim(y_lim[0], y_lim[1])\n",
    "    lw = 5\n",
    "    plt.plot(\n",
    "        list(depth),\n",
    "        list(scores),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color=\"darkred\",\n",
    "        label=\"ADA F1\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    return fig \n",
    "\n",
    "def plot_rf(scores, y_lim = [0.1, 0.9], num_trees = range(100,1000,100)):\n",
    "    \n",
    "    %matplotlib inline \n",
    "    \n",
    "    fig = figure(figsize = (15,15), dpi = 800) \n",
    "    \n",
    "    plt.rcParams.update({\"font.size\": 22})\n",
    "    plt.title(\"Random Forest F1 by Number of Trees\")\n",
    "    plt.xlabel(\"Trees\")\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.ylim(y_lim[0], y_lim[1])\n",
    "    lw = 5\n",
    "    plt.plot(\n",
    "        list(num_trees),\n",
    "        list(scores),\n",
    "        linewidth=lw,\n",
    "        alpha=0.2,\n",
    "        color=\"darkred\",\n",
    "        label=\"RF F1\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa83b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 2.957343101501465 secs.\n"
     ]
    }
   ],
   "source": [
    "from pywsd.utils import lemmatize_sentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "(X, y) = preprocess('./data/twitter_training.csv')\n",
    "\n",
    "# sentiment = SentimentIntensityAnalyzer()\n",
    "# tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# # translator = str.maketrans('', '', string.punctuation)\n",
    "# additional_stopwords = ['’', '–', '‘', '“', '”',\n",
    "# 'u', 'v', 'f', '—', '…', 'w', 'x', 'c', 'g', 'q', '•', '»', 'b', 'n']\n",
    "\n",
    "# transformed = []\n",
    "# for i in range(0,len(X)): \n",
    "#     tweet = X[i]\n",
    "#     sent = sentiment.polarity_scores(tweet)\n",
    "#     # neg[i] = sent['neg']\n",
    "#     # neu[i] = sent['neu']\n",
    "#     # pos[i] = sent['pos']\n",
    "#     # compound[i] = sent['compound']\n",
    "#     # no_punc = tweet.translate(translator)\n",
    "#     tweet = tweet.replace(\"’\", '\\'')\n",
    "#     words = tokenizer.tokenize(tweet)\n",
    "#     tweet = ' '.join([lemmatizer.lemmatize(w) for w in words])\n",
    "#     # words = [word for word in words if word not in stopwords.words('english') and word not in additional_stopwords] # remove stopwords\n",
    "#     transformed.append(tweet)\n",
    "# print('done with lemmatization')\n",
    "\n",
    "splits = train_val_test(X, y)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# svd_model = TruncatedSVD(n_components=2000, n_iter=10, random_state=42)\n",
    "# svd_transformer = Pipeline([('tfidf', vectorizer),\n",
    "#                             ('svd', svd_model)])\n",
    "\n",
    "# X_tr = svd_transformer.fit_transform(splits[\"x_tr\"])\n",
    "# X_val = svd_transformer.transform(splits[\"x_val\"])\n",
    "# X_tst = svd_transformer.transform(splits[\"x_tst\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325b9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y) = preprocess('./data/twitter_training.csv')\n",
    "splits = train_val_test(X, y)\n",
    "\n",
    "n_components = [100, 500, 1000, 2000]\n",
    "var_preserved = np.zeros(len(n_components))\n",
    "\n",
    "for (i, n) in enumerate(n_components):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    svd_model = TruncatedSVD(n_components=n, n_iter=5, random_state=42)\n",
    "    svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                                ('svd', svd_model)])\n",
    "    x_tr = svd_transformer.fit_transform(splits[\"x_tr\"])\n",
    "    x_val = svd_transformer.transform(splits[\"x_val\"])\n",
    "    x_tst = svd_transformer.transform(splits[\"x_tst\"])\n",
    "    var_preserved[i] = sum(svd_model.explained_variance_ratio_)\n",
    "    np.save(f'./tfidf_results/x_tr_{n}', x_tr)\n",
    "    np.save(f'./tfidf_results/x_val_{n}', x_val)\n",
    "    np.save(f'./tfidf_results/x_tst_{n}', x_tst)\n",
    "\n",
    "np.save('./tfidf_results/lsi_var_preserved.npy', [0.1802642, 0.38882954, 0.50905166, 0.64670595])\n",
    "np.save('./tfidf_results/lsi_time.npy', [3.8, 15.4, 31.6, 119.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88de5e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide which to load\n",
    "def load_data(n_comp = 500):\n",
    "    X_tr = np.load(f'./tfidf_results/x_tr_{n_comp}.npy', allow_pickle=True)\n",
    "    X_val = np.load(f'./tfidf_results/x_val_{n_comp}.npy', allow_pickle=True)\n",
    "    X_tst = np.load(f'./tfidf_results/x_tst_{n_comp}.npy', allow_pickle=True)\n",
    "    return (X_tr, X_val, X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26e1154a-6360-488e-be61-74781da62d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn for dist metric: euclidean\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m (metric_idx, j) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(metrics):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mknn for dist metric: \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m do_knn(X_tr, splits[\u001b[39m'\u001b[39;49m\u001b[39my_tr\u001b[39;49m\u001b[39m'\u001b[39;49m], X_val, splits[\u001b[39m'\u001b[39;49m\u001b[39my_val\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m9\u001b[39;49m, metric\u001b[39m=\u001b[39;49mj)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m (k_idx, pred) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(y_pred):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         f1_scores[metric_idx, k_idx] \u001b[39m=\u001b[39m f1_score(splits[\u001b[39m\"\u001b[39m\u001b[39my_val\u001b[39m\u001b[39m\"\u001b[39m], pred, average \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb Cell 6\u001b[0m in \u001b[0;36mdo_knn\u001b[0;34m(X_tr, y_tr, X_te, y_te, k, metric)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((k, \u001b[39mlen\u001b[39m(y_te)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m nbrs \u001b[39m=\u001b[39m NearestNeighbors(n_neighbors\u001b[39m=\u001b[39mk, metric\u001b[39m=\u001b[39mmetric, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfit(X_tr)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m distances, neighbors_list \u001b[39m=\u001b[39m nbrs\u001b[39m.\u001b[39;49mkneighbors(X_te)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m (index, neighbors) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(neighbors_list):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/steven/Repos/251Project/full_notebook/tfidf.ipynb#X54sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/neighbors/_base.py:752\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m         kwds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> 752\u001b[0m     chunked_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    753\u001b[0m         pairwise_distances_chunked(\n\u001b[1;32m    754\u001b[0m             X,\n\u001b[1;32m    755\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[1;32m    756\u001b[0m             reduce_func\u001b[39m=\u001b[39;49mreduce_func,\n\u001b[1;32m    757\u001b[0m             metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[1;32m    758\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    759\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds,\n\u001b[1;32m    760\u001b[0m         )\n\u001b[1;32m    761\u001b[0m     )\n\u001b[1;32m    763\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mball_tree\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mkd_tree\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    764\u001b[0m     \u001b[39mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1717\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 1717\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1718\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1719\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1721\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1889\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   1887\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 1889\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1435\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1433\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n\u001b[1;32m   1434\u001b[0m ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1435\u001b[0m Parallel(backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreading\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49mn_jobs)(\n\u001b[1;32m   1436\u001b[0m     fd(func, ret, s, X, Y[s], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1437\u001b[0m     \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs))\n\u001b[1;32m   1438\u001b[0m )\n\u001b[1;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1441\u001b[0m     \u001b[39m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[39m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m     np\u001b[39m.\u001b[39mfill_diagonal(ret, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from statistics import mode\n",
    "\n",
    "def do_knn(X_tr, y_tr, X_te, y_te, k, metric):\n",
    "    # regular knn for comparison\n",
    "    y_pred = np.zeros((k, len(y_te)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=metric, n_jobs=-1).fit(X_tr)\n",
    "    distances, neighbors_list = nbrs.kneighbors(X_te)\n",
    "    for (index, neighbors) in enumerate(neighbors_list):\n",
    "        for j in range(k):\n",
    "            predicted = mode(y_tr[neighbors[0:j+1]])\n",
    "            y_pred[j, index] = predicted\n",
    "                \n",
    "    return y_pred\n",
    "\n",
    "(X_tr, X_val, X_tst) = load_data(500)\n",
    "# KNN grid search\n",
    "metrics = [\"euclidean\", \"cityblock\", \"cosine\", \"correlation\"]\n",
    "f1_scores = np.zeros((4, 9))\n",
    "for (metric_idx, j) in enumerate(metrics):\n",
    "    print(f'knn for dist metric: {j}')\n",
    "    y_pred = do_knn(X_tr, splits['y_tr'], X_val, splits['y_val'], 9, metric=j)\n",
    "    for (k_idx, pred) in enumerate(y_pred):\n",
    "        f1_scores[metric_idx, k_idx] = f1_score(splits[\"y_val\"], pred, average = 'weighted')\n",
    "\n",
    "np.save('./tfidf_results/knn_grid.npy', f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfbcf564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814051709806621"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_tr, X_val, X_tst) = load_data(500)\n",
    "knn_results = np.load('./tfidf_results/knn_grid.npy')\n",
    "row = knn_results.argmax(axis=0)[0]\n",
    "col = knn_results.argmax(axis=1)[0]\n",
    "metric = metrics[row]\n",
    "k = col+1\n",
    "best_knn = KNeighborsClassifier(n_neighbors=k, metric=metric, n_jobs=-1).fit(X_tr, splits[\"y_tr\"])\n",
    "best_preds = best_knn.predict(X_tst)\n",
    "best_score = f1_score(splits[\"y_tst\"], best_preds, average = 'weighted')\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3fba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for kernel poly\n",
      "C=0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for kernel rbf\n",
      "C=0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for kernel sigmoid\n",
      "C=0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for linear\n",
      "C=0.03125\n",
      "C=0.0625\n",
      "C=0.125\n",
      "C=0.25\n",
      "C=0.5\n",
      "C=1.0\n",
      "C=2.0\n",
      "C=4.0\n",
      "C=8.0\n",
      "C=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/opt/miniconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "(X_tr, X_val, X_tst) = load_data(500)\n",
    "\n",
    "C = [np.float_power(2,i) for i in range(-5,5)]\n",
    "kernels = ['poly', 'rbf', 'sigmoid'] \n",
    "kernel_score_dict = {\"poly\": [], \"rbf\": [], \"sigmoid\": []}\n",
    "\n",
    "for kern in kernels:\n",
    "    print(f'validation for kernel {kern}')\n",
    "    for reg in C: \n",
    "        print(f'C={reg}')\n",
    "        clf = SVC(C = reg, kernel = kern, max_iter = 100).fit(X_tr, splits['y_tr'])\n",
    "        preds = clf.predict(X_val)\n",
    "        score = f1_score(splits['y_val'], preds, average = 'weighted') \n",
    "        kernel_score_dict[kern].append(score)\n",
    "        \n",
    "np.save('./tfidf_results/svm_kernel_grid.npy', kernel_score_dict)\n",
    "\n",
    "## SVM LINEAR\n",
    "\n",
    "svc_scores = []\n",
    "print(f'validation for linear')\n",
    "for rg in C: \n",
    "    print(f'C={rg}')\n",
    "    clf = LinearSVC(C = rg).fit(X_tr, splits['y_tr'])\n",
    "    preds = clf.predict(X_val)\n",
    "    score = f1_score(splits['y_val'], preds, average = \"weighted\")\n",
    "    svc_scores.append(score)\n",
    "np.save('./tfidf_results/linear_svm_grid.npy', svc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3750c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poly': [0.061287283775189834, 0.06620877049156744, 0.06945648433969527, 0.07573702975006773, 0.08055489903310457, 0.09226769470093514, 0.0974755275387486, 0.10355492303658524, 0.11208389963585734, 0.11745273171254758], 'rbf': [0.28546685547050404, 0.26802587356984603, 0.26009665560489054, 0.26496595174895765, 0.3018738351682088, 0.27499465905024895, 0.21873220703103866, 0.335625150757089, 0.2875576895015411, 0.30405327965432555], 'sigmoid': [0.16399039189720707, 0.14761816763432117, 0.1982724881460499, 0.31909978363965547, 0.22572303502514635, 0.30425790174423156, 0.31796847858428245, 0.3266918508467188, 0.3200139701755483, 0.31601892386607244]}\n",
      "[0.47527776 0.47939911 0.48222552 0.48546006 0.4867708  0.48735393\n",
      " 0.48747696 0.48774593 0.48769687 0.48777315]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "(X_tr, X_val, X_tst) = load_data(2000)\n",
    "kernel_results = np.load('./tfidf_results/svm_kernel_grid.npy', allow_pickle=True)\n",
    "linear_results = np.load('./tfidf_results/linear_svm_grid.npy', allow_pickle=True)\n",
    "\n",
    "print(kernel_results)\n",
    "print(linear_results)\n",
    "\n",
    "print(linear_results.argmax())\n",
    "\n",
    "# best_svm = LinearSVC(C = 2**5).fit(X_tr, splits[\"y_tr\"])\n",
    "# best_preds = best_svm.predict(X_tst)\n",
    "# best_score = f1_score(splits[\"y_tst\"], best_preds, average = 'weighted')\n",
    "# print(best_score)\n",
    "\n",
    "# best_svm = LinearSVC(C = 2**5).fit(splits[\"x_tr\"], splits[\"y_tr\"])\n",
    "# best_preds = best_svm.predict(splits[\"x_tst\"])\n",
    "# best_score = f1_score(splits[\"y_tst\"], best_preds, average = 'weighted')\n",
    "# print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b645fdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120973300822184"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NB test\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "full_X_tr = vectorizer.fit_transform(splits[\"x_tr\"])\n",
    "full_X_val = vectorizer.transform(splits[\"x_val\"])\n",
    "full_X_tst = vectorizer.transform(splits[\"x_tst\"])\n",
    "\n",
    "mnb = MultinomialNB().fit(full_X_tr, splits['y_tr'])\n",
    "mnb_pred = mnb.predict(full_X_tst)\n",
    "best_score = f1_score(splits[\"y_tst\"], mnb_pred, average = 'weighted')\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c063823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF tuning\n",
    "(X_tr, X_val, X_tst) = load_data(500)\n",
    "n_estimators = list(range(100, 1000, 100))\n",
    "rf_scores = []\n",
    "for i in n_estimators: \n",
    "    rf = RandomForestClassifier(n_estimators = i, n_jobs = -1).fit(X_tr, splits[\"y_tr\"])\n",
    "    preds = rf.predict(X_val) \n",
    "    score = f1_score(splits[\"y_val\"], preds, average = 'weighted')\n",
    "    rf_scores.append(score)\n",
    "\n",
    "np.save('./tfidf_results/rf_grid.npy', rf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ae02b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189183921459836"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF test\n",
    "(X_tr, X_val, X_tst) = load_data(500)\n",
    "rf_list = np.load('rf_scores.npy', allow_pickle = True) \n",
    "T = n_estimators[np.argmax(rf_list)] # 600 estimators\n",
    "\n",
    "best_rf = RandomForestClassifier(n_estimators = T, n_jobs = -1).fit(X_tr, splits[\"y_tr\"]) \n",
    "best_rf_preds = best_rf.predict(X_tst)\n",
    "rf_score = f1_score(splits[\"y_tst\"], best_rf_preds, average = 'weighted')\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea7a788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfbag = BaggingClassifier(n_estimators = 600, n_jobs = -1).fit(X_tr, splits[\"y_tr\"]) \n",
    "rfbag_preds = rfbag.predict(X_tst) \n",
    "rfbag_score = f1_score(splits[\"y_tst\"], rfbag_preds, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e15732f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777532315389841"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4a5add0ac853a12aeb3c19b79b16ea7446d0458a836ed5ca60cbf7b1d6f7838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
